# game-deal-tracker/crawlers/steam_crawler.py

import requests
import re
import time
from datetime import datetime
from bs4 import BeautifulSoup
from config.database import get_db_context 
from db.models import Deal, SteamMetadata 
from db.crud import upsert_deal 
from typing import List, Dict, Set, Tuple

# ìŠ¤íŒ€ ê²€ìƒ‰ API ì—”ë“œí¬ì¸íŠ¸
# specials=1 (í• ì¸ ìƒí’ˆ), cc=kr (í•œêµ­ ê¸°ì¤€), l=koreana (ì–¸ì–´ ì„¤ì •)
# count=1ì€ ì´ˆê¸° ì´ ê°œìˆ˜ íŒŒì•…ì„ ìœ„í•¨
STEAM_SEARCH_URL = "https://store.steampowered.com/search/results/?query&start={start}&count=100&dynamic_data=&sort_by=_ASC&snr=1_7_7_151_7&infinite=1&specials=1&cc=kr&l=koreana"
STEAM_COUNT_URL = "https://store.steampowered.com/search/results/?query&start=0&count=1&dynamic_data=&sort_by=_ASC&snr=1_7_7_151_7&infinite=1&specials=1&cc=kr&l=koreana"

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
}

def get_total_sales_count() -> int:
    """APIì— ì ‘ì†í•˜ì—¬ í˜„ì¬ ì„¸ì¼ ì¤‘ì¸ ê²Œì„ì˜ ì´ ê°œìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        response = requests.get(STEAM_COUNT_URL, headers=HEADERS)
        response.raise_for_status()
        
        # JSON ë””ì½”ë”© ì‹œë„
        try:
            data = response.json()
        except requests.exceptions.JSONDecodeError:
            print("  - WARNING: Initial API call returned non-JSON. Possible temporary block. Returning 0.")
            return 0
        
        # total_count í•„ë“œì—ì„œ ì´ ê°œìˆ˜ ì¶”ì¶œ
        return data.get('total_count', 0)
        
    except requests.exceptions.RequestException as e:
        print(f"ERROR: Failed to fetch initial sales count: {e}")
        return 0

def fetch_steam_sales(limit=300) -> List[dict]:
    """ìŠ¤íŒ€ í• ì¸ ê²Œì„ ëª©ë¡ì„ ê°€ì ¸ì™€ íŒŒì‹±í•©ë‹ˆë‹¤."""
    total_count = get_total_sales_count()
    if total_count == 0:
        print("  - No sales found or initial API fetch failed.")
        return []
        
    actual_limit = min(limit, total_count)
    all_deals = []
    start = 0
    
    print(f"ğŸš‚ Fetching Steam sales (Target: {actual_limit} of {total_count} items)...")

    while len(all_deals) < actual_limit:
        try:
            url = STEAM_SEARCH_URL.format(start=start)
            response = requests.get(url, headers=HEADERS)
            
            if response.status_code != 200:
                print(f"  - Failed to fetch Steam data: Status {response.status_code}. Stopping.")
                break
            
            try:
                data = response.json()
            except requests.exceptions.JSONDecodeError:
                print("  - CRITICAL: JSON decoding failed during bulk fetch. Stopping.")
                break
                
            html_content = data.get('results_html')
            if not html_content:
                break
                
            soup = BeautifulSoup(html_content, 'html.parser')
            rows = soup.select('a.search_result_row')
            
            if not rows:
                break
                
            for row in rows:
                deal_info = parse_steam_row(row)
                if deal_info:
                    all_deals.append(deal_info)
            
            start += len(rows)
            print(f"  - Fetched {len(all_deals)} deals so far...")
            
            time.sleep(1)
            
        except Exception as e:
            print(f"âŒ Error during Steam crawling loop: {e}. Stopping.")
            break
            
    return all_deals[:actual_limit]

def parse_steam_row(row):
    """HTML í–‰ í•˜ë‚˜ì—ì„œ ê²Œì„ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³ , deal_dataì™€ meta_dataë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤. (íŒŒì‹± ë¡œì§ ìˆ˜ì •ë¨)"""
    title = "Unknown Title"
    try:
        title_tag = row.select_one('span.title')
        if not title_tag: return None
        title = title_tag.text.strip()
        url = row['href']
        
        # App ID ì¶”ì¶œ
        app_id_str = row.get('data-ds-appid')
        steam_app_id = int(app_id_str.split(',')[0]) if app_id_str else None
        if not steam_app_id: return None
        
        # ì´ë¯¸ì§€ URL
        img_tag = row.select_one('div.search_capsule img')
        image_url = img_tag['src'].replace("capsule_231x87", "header") if img_tag and img_tag.get('src') else None

        # --- ğŸš¨ [í•µì‹¬ ìˆ˜ì •: ê°€ê²© ìš”ì†Œ ì„ íƒì ë³€ê²½] ---
        discount_block = row.select_one('.discount_block.search_discount_block')
        
        if not discount_block: return None 

        discount_rate = int(discount_block.get('data-discount', '0'))
        
        # 1. ì •ê°€: .discount_original_price í´ë˜ìŠ¤ ì‚¬ìš© (strike íƒœê·¸ ëŒ€ì‹ )
        original_price_tag = discount_block.select_one('.discount_original_price')
        
        # 2. í• ì¸ê°€: .discount_final_price í´ë˜ìŠ¤ ì‚¬ìš©
        final_price_tag = discount_block.select_one('.discount_final_price')


        if not original_price_tag or not final_price_tag:
             return None

        # ê°€ê²© ìˆ«ì ë³€í™˜ ë¡œì§ (â‚© ë° , ì œê±°)
        try:
            regular_price_text = original_price_tag.get_text(strip=True)
            regular_price = float(re.sub(r'[^\d.]', '', regular_price_text.replace(',', '')))
            
            final_price_text = final_price_tag.get_text(strip=True)
            sale_price = float(re.sub(r'[^\d.]', '', final_price_text.replace(',', '')))
            
        except ValueError:
            return None
        
        # ë¦¬ë·° ì •ë³´ ì¶”ì¶œ
        review_summary, positive_percent, total_reviews = "", 0, 0
        review_span = row.select_one('span.search_review_summary')
        if review_span:
            tooltip = review_span.get('data-tooltip-html', '')
            review_parts = tooltip.split('<br>')
            if len(review_parts) > 0:
                review_summary = review_parts[0]
                
            match = re.search(r'(\d+)%[^0-9]+([\d,]+)', tooltip)
            if match:
                positive_percent = int(match.group(1))
                total_reviews = int(match.group(2).replace(',', ''))
        
        return {
            "deal_data": { 
                "platform": "Steam",
                "title": title,
                "url": url,
                "image_url": image_url,
                "regular_price": regular_price,
                "sale_price": sale_price,
                "discount_rate": discount_rate,
                "deal_type": "Sale", 
                "is_active": True,
                "end_date": None, 
            },
            "meta_data": { 
                "steam_app_id": steam_app_id,
                "review_summary": review_summary,
                "positive_review_percent": positive_percent,
                "total_reviews": total_reviews
            }
        }
    except Exception as e:
        print(f"âŒ CRITICAL PARSING ERROR for Steam deal: {e} (Title: {title})")
        return None

# --- main.pyê°€ ì„í¬íŠ¸í•˜ëŠ” ìµœì¢… ì§„ì…ì  í•¨ìˆ˜ ---
def crawl_steam():
    deals_structured = fetch_steam_sales()
    if not deals_structured:
        return 0
        
    count_saved = 0
    count_updated = 0
    
    with get_db_context() as db:
        for item in deals_structured:
            try:
                result = upsert_deal(
                    db,
                    deal_data=item["deal_data"],
                    metadata_model=SteamMetadata,
                    metadata_data=item["meta_data"]
                )
                
                if result == "created": count_saved += 1
                else: count_updated += 1
                
            except Exception as e:
                db.rollback() 
                print(f"âš ï¸ Steam DB Error ({item['deal_data'].get('title', 'Unknown')}): {e}")
                continue
                
        db.commit()
        print(f"âœ… Steam Crawler Finished: Added {count_saved}, Updated {count_updated}.")
        return count_saved

if __name__ == "__main__":
    crawl_steam()